# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o
OPENAI_BASE_URL=https://api.openai.com/v1

# Jira Configuration
JIRA_BASE_URL=https://jirastage.deltaww.com
JIRA_AUTH_TOKEN=YWx2aXMuYWRtaW46UGFyYTk0Nzg=

# Application Configuration
NODE_ENV=development
LOG_LEVEL=info
PORT=3000

# CORS Configuration
CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# Testing Configuration
TEST_MODE=false

# Agent Version Configuration
# Set to 'true' to use V2 agents with enhanced prompt engineering
# V2 agents provide better context handling and quality evaluation
USE_V2_AGENTS=true

# ===================================================================
# PostgreSQL Database Configuration
# ===================================================================
# LangGraph's LangMem uses PostgreSQL for checkpoint persistence
# This enables conversation continuity across multiple interactions
# using thread_id (Project ID) as the conversation identifier
#
# Option 1: Use DATABASE_URL (recommended)
DATABASE_URL=postgresql://user:password@localhost:5432/jira_cs

# Option 2: Use individual variables (if DATABASE_URL is not set)
# DATABASE_HOST=localhost
# DATABASE_PORT=5432
# DATABASE_NAME=jira_cs
# DATABASE_USER=postgres
# DATABASE_PASSWORD=your_password_here
# DATABASE_SSL=false

# Database Connection Pool Settings
DATABASE_MAX_CONNECTIONS=10
DATABASE_IDLE_TIMEOUT=30000
DATABASE_CONNECTION_TIMEOUT=5000

# ===================================================================
# Context Management Configuration
# ===================================================================
# NOTE: The system uses LangGraph's native LangMem for primary context persistence.
# The following custom context compression settings are kept for additional analytics
# and custom context features beyond LangMem's built-in capabilities.

# Trigger compression when conversation turns exceed this number
CONTEXT_COMPRESSION_TURN_THRESHOLD=5

# Trigger compression when total tokens exceed this number
CONTEXT_COMPRESSION_TOKEN_THRESHOLD=10000

# Number of recent conversation turns to keep uncompressed
CONTEXT_KEEP_RECENT_TURNS=3

# LLM model to use for context compression
CONTEXT_COMPRESSION_MODEL=gpt-4o-mini

# Maximum tokens for compressed context
CONTEXT_MAX_COMPRESSED_TOKENS=3000

# ===================================================================
# LangMem Behavior
# ===================================================================
# LangMem automatically manages conversation state using PostgreSQL checkpoints
# No additional configuration needed - just ensure DATABASE_URL is set correctly
# Thread continuity is maintained using Project ID as the thread_id identifier